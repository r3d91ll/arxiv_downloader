# Configuration for backfilling ALL arXiv papers - no category filtering

download:
  rate_limit: 3.0  # 3 seconds between each download
  timeout: 60
  chunk_size: 8192
  max_retries: 5
  retry_delay: 10.0
  
  # Pacing controls - matching your rhythm
  batch_size: 10  # Pause after every 10 downloads
  batch_pause: 10.0  # 10 second pause after each batch
  
  # Session controls
  session_pause_after: 100  # Take a break every 100 downloads
  session_pause_duration: 60.0  # 1 minute break
  
  # Daily limit
  daily_limit: 1800  # Safe daily limit

directories:
  base_dir: "/mnt/data/arxiv_data"
  pdf_subdir: "pdf"
  metadata_subdir: "metadata"

api:
  base_url: "http://export.arxiv.org/api/query"
  max_results_per_query: 500  # Smaller batches for stability
  default_sort_by: "submittedDate"
  default_sort_order: "ascending"  # Start from oldest

logging:
  level: "INFO"
  format: "%(asctime)s - %(levelname)s - %(message)s"
  file: "logs/backfill_all.log"

jobs:
  backfill_all_1991:
    enabled: true
    bulk_start_year: 1991  # arXiv started in 1991
    bulk_max_per_month: 500  # Process in smaller monthly chunks
    # NO categories field = download everything!
    
  backfill_all_recent:
    enabled: true
    bulk_start_year: 2020  # More recent papers
    bulk_max_per_month: 1000  # Can handle more per month for recent years
    # NO categories field = download everything!