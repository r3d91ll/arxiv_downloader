# Safe configuration for long-running backfill - respects ArXiv and runs 24/7 responsibly

download:
  rate_limit: 3.0  # 3 seconds between each download
  timeout: 60
  chunk_size: 8192
  max_retries: 5
  retry_delay: 10.0
  
  # Pacing controls - matching your rhythm
  batch_size: 10  # Pause after every 10 downloads
  batch_pause: 10.0  # 10 second pause after each batch
  
  # Session controls
  session_pause_after: 100  # Take a break every 100 downloads
  session_pause_duration: 60.0  # 1 minute break
  
  # Daily limit - let's calculate a safe number
  # With 3s between downloads + 10s every 10 downloads:
  # Average time per download = 3s + (10s/10) = 4s
  # In 24 hours = 86400s / 4s = 21600 theoretical max
  # But with breaks and safety margin, let's do 1500-2000 per day
  daily_limit: 1800  # Safe daily limit

directories:
  base_dir: "arxiv_papers"
  pdf_subdir: "pdf"
  metadata_subdir: "metadata"

api:
  base_url: "http://export.arxiv.org/api/query"
  max_results_per_query: 500  # Smaller batches for stability
  default_sort_by: "submittedDate"
  default_sort_order: "ascending"  # Start from oldest

logging:
  level: "INFO"
  format: "%(asctime)s - %(levelname)s - %(message)s"
  file: "logs/backfill_safe.log"

jobs:
  safe_backfill:
    enabled: true
    bulk_start_year: 2010
    bulk_max_per_month: 500  # Process in smaller monthly chunks
    categories:  # AI/ML categories
      - "cs.AI"
      - "cs.LG"
      - "cs.CL"
      - "cs.CV"
      - "cs.NE"
      - "stat.ML"
      - "cs.IR"
      - "cs.RO"